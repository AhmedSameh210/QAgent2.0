# -*- coding: utf-8 -*-
"""Qagent_Chromadb.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nnQ5eKj-Lv6IJ3KkJi18wX47q7rymzuy

#Installments
"""

pip install transformers torch chromadb pandas

import chromadb
from chromadb.config import Settings
import pandas as pd
from transformers import AutoTokenizer, AutoModel
import torch

"""# Read Csv"""

df_py = pd.read_csv("/content/final_python_dataset.csv")
df_c = pd.read_csv("/content/final_C++_dataset.csv")
df_java = pd.read_csv("/content/final_java_dataset.csv")
df_script = pd.read_csv("/content/final_Java_script_dataset.csv")

"""# GraphCodeBert"""

tokenizer = AutoTokenizer.from_pretrained('microsoft/graphcodebert-base')
model = AutoModel.from_pretrained('microsoft/graphcodebert-base')

def embed_code(code_snippet,model,tokenizer):
  #Tokenize Input
    inputs = tokenizer(code_snippet, return_tensors='pt', padding=True, truncation=True)
    #Disable backward pass since we are only interested in Forward Pass
    with torch.no_grad():
      #Unpack Input
        outputs = model(**inputs)
        #averaging the hidden states across all tokens in the sequence
        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()
    return embedding

df_py["embedding"]= df_py["code"].apply(embed_code)
df_c["embedding"]= df_c["code"].apply(embed_code)
df_java["embedding"]= df_java["code"].apply(embed_code)
df_script["embedding"]= df_script["code"].apply(embed_code)

"""# ChromadB"""

def initialize_chroma_client(collection_name, persist_directory):
    client = chromadb.PersistentClient(path=persist_directory)
    collection = client.create_collection(name=collection_name)
    return client, collection

def add_embeddings_to_collection(df, collection):
    for index, row in df.iterrows():
        collection.add(
            embeddings=[row['embedding']],
            documents=[row['code']],
            ids=[str(index)]
        )

"""# PY"""

import os
db_path = "/content/VectorDATABASEPY"
os.makedirs(db_path,exist_ok=True)
os.chmod("/content/VectorDATABASEPY", 0o755)
client1, collection1 = initialize_chroma_client("codePY",db_path )
add_embeddings_to_collection(df_py, collection1)

!zip -r //content/VectorDATABASEPY.zip /content/VectorDATABASEPY



"""# C++"""

import os
db_path = "/content/VectorDATABASEBC++"
os.makedirs(db_path,exist_ok=True)
os.chmod("/content/VectorDATABASEBC++", 0o755)
client2, collection2 = initialize_chroma_client("codeC",db_path )
add_embeddings_to_collection(df_c, collection2)

!zip -r ///content/VectorDATABASEBC++.zip //content/VectorDATABASEBC++

"""# JAVA"""

import os
db_path = "/content/VectorDATABASEJAVA"
os.makedirs(db_path,exist_ok=True)
os.chmod("/content/VectorDATABASEJAVA", 0o755)
client3, collection3 = initialize_chroma_client("CODEJAVA",db_path )
add_embeddings_to_collection(df_java, collection3)

!zip -r ///content/VectorDATABASEJAVA //content/VectorDATABASEJAVA

"""# JAVASCRIPT"""

import os
db_path = "/content/VectorDATABASEJAVASCRIPT"
os.makedirs(db_path,exist_ok=True)
os.chmod("/content/VectorDATABASEJAVASCRIPT", 0o755)
client4, collection4 = initialize_chroma_client("CODEJSCRIPT",db_path )
add_embeddings_to_collection(df_script, collection4)

!zip -r /content/VectorDATABASEJAVASCRIPT.zip /content/VectorDATABASEJAVASCRIPT

print(f"Collection 3 count: {collection1.count()}")
print(f"Collection 4 count: {collection2.count()}")

/content/drive/MyDrive/Python_VDB_Graph_Code_BERT

import chromadb
db_path = '/content/drive/MyDrive/Python_VDB_Graph_Code_BERT'
#os.makedirs(db_path, exist_ok=True)

client = chromadb.PersistentClient(path=db_path)
collections = client.list_collections()
collection = client.get_collection(name="codePY")

# Update the function to return results sorted by similarity
def get_similar_code_by_description(description, collection, model, tokenizer, top_n=5):
    try:
        embedding = embed_code(description, model, tokenizer)

        embedding_list = embedding.flatten().tolist()

        # Query the collection
        results = collection.query(
            query_embeddings=[embedding_list],  # Pass embedding as a list
            n_results=top_n
        )
        print("Query Results:", results)

        similar_code_snippets = []
        for item_id, document, distance in zip(
            results["ids"][0],
            results["documents"][0],
            results["distances"][0]
        ):
            # Append the results including the distance value
            similar_code_snippets.append({
                "id": item_id,
                "description": document,
                "distance": distance  # Store the distance value
            })

        # Sort the list by distance
        similar_code_snippets.sort(key=lambda x: x["distance"])

        for snippet in similar_code_snippets:
            print(f"ID: {snippet['id']}")
            print(f"Description: {snippet['description']}")
            print(f"Distance: {snippet['distance']}")
            print("-" * 50)

        return similar_code_snippets

    except Exception as e:
        print(f"An error occurred: {e}")
        return []

code = "def sum(x,y) : return x+y"
get_similar_code_by_description(code,collection,model,tokenizer,10)
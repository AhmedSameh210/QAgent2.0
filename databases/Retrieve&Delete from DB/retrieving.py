# -*- coding: utf-8 -*-
"""Retrieving.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FqAvJF3mLIJ0Fez-ILIOprwJd55Yjaxe
"""

!pip install chromadb

import chromadb
from transformers import AutoTokenizer, AutoModel
import torch
import random
from transformers import AutoModelForCausalLM

from google.colab import drive
drive.mount('/content/drive')

"""# Code

(Codet5 , graph code bert , qween)
"""

model_codet5_n = "Salesforce/codet5-base"
tokenizer_codet5 = AutoTokenizer.from_pretrained(model_codet5_n)
model_codet5 = AutoModel.from_pretrained(model_codet5_n)

#Code t5 code (Sameh Notebook)
def get_embeddings_CodeT5_Code(code,model,tokenizer):
    inputs = tokenizer(code, return_tensors="pt", padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        encoder_outputs = model.encoder(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"]
        )
    return encoder_outputs.last_hidden_state.mean(dim=1).squeeze().numpy()

tokenizer_graphBert = AutoTokenizer.from_pretrained('microsoft/graphcodebert-base')
model_graphBert = AutoModel.from_pretrained('microsoft/graphcodebert-base')

#Graph Bert code (Jannah Notebook)
def get_embeddings_GraphBert_Code(code_snippet,model,tokenizer):

    inputs = tokenizer(code_snippet, return_tensors='pt', padding=True, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()
    return embedding

#Qween code (Amjad Notebook)
model_qween_n = "Qwen/Qwen2.5-Coder-7B-Instruct"
model_qween = AutoModelForCausalLM.from_pretrained(
    model_qween_n,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer_qween = AutoTokenizer.from_pretrained(model_qween_n)

def get_embeddings_Qween_Codee(texts , model ,tokenizer):

    prompt = texts
    messages = [
        {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant in C++."},
        {"role": "user", "content": prompt}
    ]

    # Convert messages to formatted text
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    # Tokenize input
    inputs = tokenizer([text], return_tensors="pt")
    print(model)
    with torch.no_grad():
        outputs = model(**inputs, output_hidden_states=True)

    # Mean pooling to get embeddings
    embeddings = outputs.hidden_states[-1].float().mean(dim=1).cpu().numpy()

    return embeddings

def Get_similar_Code(code,model_name , model, tokenizer, db_path, collection_name):
    try:
        client = chromadb.PersistentClient(path=db_path)
        collection = client.get_collection(name=collection_name)

        if model_name.lower() == "graphcodebert":
            embedding = get_embeddings_GraphBert_Code(code, model,tokenizer)
        elif model_name.lower() == "codet5":
            embedding = get_embeddings_CodeT5_Code(code, model,tokenizer)
        elif model_name.lower() == "qween":
            embedding = get_embeddings_Qween_Codee(code, model,tokenizer)
        else:
            raise ValueError(f"Unsupported model name: {model_name}")

        embedding_list = embedding.flatten().tolist()

        total_documents = collection.count()

        results = collection.query(
            query_embeddings=[embedding_list],
            n_results=total_documents
        )

        # Fetch distances and normalize them
        distances = results["distances"][0]
        min_distance = min(distances)
        max_distance = max(distances)

        def normalize_distance(distance):
            # Normalize distances to the range [0, 1]
            return 1 - ((distance - min_distance) / (max_distance - min_distance))

        similar_code_snippets = [
            {
                "id": item_id,
                "code": document,
                "similarity": normalize_distance(distance)
            }
            for item_id, document, distance in zip(
                results["ids"][0],
                results["documents"][0],
                results["distances"][0]
            )
        ]

        # Sort by cosine similarity (descending order)
        similar_code_snippets.sort(key=lambda x: x["similarity"], reverse=True)

        for snippet in similar_code_snippets:
            print(f"ID: {snippet['id']}")
            print(f"code: {snippet['code']}")
            print(f"Cosine Similarity (0-1): {snippet['similarity']:.4f}")
            print("-" * 50)

        return similar_code_snippets

    except Exception as e:
        print(f"An error occurred: {e}")
        return []

code =  """
 def div_list(nums1,nums2):
  result = map(lambda x, y: x / y, nums1, nums2)
  return list(result)
  """

# Call the function code t5
similar_code = Get_similar_Code(code, "codet5",model_codet5, tokenizer_codet5,"/content/drive/MyDrive/Chroma DB/python_code/python_codet5_code_Vdb","python_codet5_code_collection")

# Call the function code t5
similar_code = Get_similar_Code(code, "qween",model_qween, tokenizer_qween,"/content/drive/MyDrive/Chroma DB/python_code/python-qwen-VectorDB","pythonVectorDataBase-Qwen2.5-Coder-7B")

text = """
def div_list(nums1,nums2):
  result = map(lambda x, y: x / y, nums1, nums2)
  return list(result)


[Test Units List]


['assert div_list([4,5,6],[1, 2, 3])==[4.0,2.5,2.0]', 'assert div_list([3,2],[1,4])==[3.0, 0.5]', 'assert div_list([90,120],[50,70])==[1.8, 1.7142857142857142]']
"""

print(text.split("[Test Units List]")[1])

# Call the function graph code bert
similar_code = Get_similar_Code(code, "graphcodebert",model_graphBert, tokenizer_graphBert,"/content/drive/MyDrive/Chroma DB/python_code/Python_VDB_Graph_Code_BERT","codePY")

"""# DESC

Falcon , codet5
"""

#Code t5 desc (Sameh Notebook)
def get_embeddings_CodeT5_Desc(code,model,tokenizer):
    inputs = tokenizer(code, return_tensors="pt", padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        encoder_outputs = model.encoder(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"]
        )
    return encoder_outputs.last_hidden_state.mean(dim=1).squeeze().numpy()

tokenizer_falcon = AutoTokenizer.from_pretrained("Rocketknight1/falcon-rw-1b")
model_falcon = AutoModel.from_pretrained("Rocketknight1/falcon-rw-1b")

#Falcon desc (Rawan Notebook)
def get_embeddings_Falcon_Desc(texts,model, tokenizer):

    inputs = tokenizer([texts], return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model(**inputs, output_hidden_states=True)

    embeddings = outputs.hidden_states[-1].float().mean(dim=1).cpu().numpy()
    return embeddings

def Get_similar_Desc(description,model_name , model, tokenizer, db_path, collection_name):
    try:
        client = chromadb.PersistentClient(path=db_path)
        collection = client.get_collection(name=collection_name)

        if model_name.lower() == "falcon":
            embedding = get_embeddings_Falcon_Desc(description, model,tokenizer)
        elif model_name.lower() == "codet5":
            embedding = get_embeddings_CodeT5_Desc(description, model,tokenizer)
        else:
            raise ValueError(f"Unsupported model name: {model_name}")

        embedding_list = embedding.flatten().tolist()

        total_documents = collection.count()

        results = collection.query(
            query_embeddings=[embedding_list],
            n_results=total_documents
        )

        distances = results["distances"][0]
        min_distance = min(distances)
        max_distance = max(distances)

        def normalize_distance(distance):
            # Normalize distances to the range [0, 1]
            return 1 - ((distance - min_distance) / (max_distance - min_distance))

        # Process the results
        similar_code_snippets = [
            {
                "id": item_id,
                "description": document,
                "similarity": normalize_distance(distance)
            }
            for item_id, document, distance in zip(
                results["ids"][0],
                results["documents"][0],
                results["distances"][0]
            )
        ]

        # Sort by cosine similarity (descending order)
        similar_code_snippets.sort(key=lambda x: x["similarity"], reverse=True)

        for snippet in similar_code_snippets:
            print(f"ID: {snippet['id']}")
            print(f"Description: {snippet['description']}")
            print(f"Cosine Similarity (0-1): {snippet['similarity']:.4f}")
            print("-" * 50)

        return similar_code_snippets

    except Exception as e:
        print(f"An error occurred: {e}")
        return []

description = "Write a function to find the length of the longest sub-sequence such that elements in the subsequences are consecutive integers."

# Call the function
similar_snippets = Get_similar_Desc(description, "falcon",model_falcon, tokenizer_falcon,"/content/drive/MyDrive/Chroma DB/Desc_Falcon/Chroma_DB_Py_Desc","Python-VectorDB-Desc-Falcon")

# Call the function code t5
similar_code = Get_similar_Desc(description, "codet5",model_codet5, tokenizer_codet5,"/content/drive/MyDrive/Chroma DB/python_codet5_disc_Vdb","python_codet5_disc_collection")

"""# Delete Indexing"""

import random
import chromadb
import os

def delete_random_indices(db_path, collection_name, num_to_delete, output_dir="deleted_ids_folder"):

    # Step 1: Load the Chroma DB client using PersistentClient with the correct path
    client = chromadb.PersistentClient(path=db_path)

    # Step 2: Retrieve the collection
    collection = client.get_collection(collection_name)

    # Step 3: Get all document IDs from the collection by fetching all documents
    all_indices = collection.get(ids=None)  # This retrieves all documents, including their IDs
    document_ids = all_indices['ids']  # Extract document IDs

    # Count the number of documents before deletion
    initial_count = len(document_ids)
    print(f"Initial number of documents: {initial_count}")

    if len(document_ids) < num_to_delete:
        print(f"Not enough indices to delete. There are only {len(document_ids)} indices.")
        return

    # Step 4: Choose a random number of indices to delete
    indices_to_delete = random.sample(document_ids, num_to_delete)

    # Step 5: Delete the selected indices
    for index in indices_to_delete:
        collection.delete(ids=[index])

    # Step 6: Save deleted IDs to a file in the specified directory
    os.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists
    output_file_path = os.path.join(output_dir, "deleted_ids.txt")  # Construct the file path

    with open(output_file_path, "a") as file:
        file.write("\n".join(indices_to_delete) + "\n")

    # Step 7: Count the number of documents after deletion
    all_indices_after_deletion = collection.get(ids=None)  # Fetch remaining documents
    remaining_document_ids = all_indices_after_deletion['ids']
    remaining_count = len(remaining_document_ids)

    print(f"Deleted {num_to_delete} random indices.")
    print(f"Remaining number of documents: {remaining_count}")
    print(f"Deleted IDs saved to {output_file_path}.")

def delete_ids_that_from_file(db_path, collection_name, input_file="deleted_ids_folder/deleted_ids.txt"):
    """
    Reads IDs from a file and deletes them from a specified collection.

    Args:
        db_path (str): Path to the Chroma DB database.
        collection_name (str): Name of the collection to modify.
        input_file (str): Path to the file containing IDs to delete.
    """
    # Step 1: Ensure the file exists
    if not os.path.exists(input_file):
        print(f"File {input_file} does not exist.")
        return

    # Step 2: Load the IDs from the file
    with open(input_file, "r") as file:
        ids_to_delete = file.read().splitlines()

    if not ids_to_delete:
        print("No IDs found in the file to delete.")
        return

    print(f"Loaded {len(ids_to_delete)} IDs from {input_file}.")

    # Step 3: Load the Chroma DB client using PersistentClient with the correct path
    client = chromadb.PersistentClient(path=db_path)

    # Step 4: Retrieve the collection
    collection = client.get_collection(collection_name)

    # Step 5: Delete the IDs from the collection
    for doc_id in ids_to_delete:
        collection.delete(ids=[doc_id])

    print(f"Deleted {len(ids_to_delete)} IDs from the collection '{collection_name}'.")

db_path = "/content/drive/MyDrive/Chroma DB/Desc_Falcon/Chroma_DB_Py_Desc"

client_py = chromadb.PersistentClient(path=db_path)

collections = client_py.list_collections()

collections

collection_name = "Python-VectorDB-Desc-Falcon"

removed_count = delete_random_indices(db_path, collection_name,100)

delete_ids_that_from_file(
    db_path="/content/drive/MyDrive/Chroma DB/Desc_Falcon/python_codet5_code_Vdb",
    collection_name="python_codet5_code_collection"
)

client = chromadb.PersistentClient(path="/content/drive/MyDrive/Chroma DB/Desc_Falcon/python_codet5_code_Vdb")
collection = client.get_collection("python_codet5_code_collection")

remaining_docs = collection.get()

print("Remaining documents:", remaining_docs)